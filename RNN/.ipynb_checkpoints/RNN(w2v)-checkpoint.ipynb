{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155cde54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "196/196 [==============================] - 64s 306ms/step - loss: 0.6769 - acc: 0.5713 - val_loss: 0.6738 - val_acc: 0.5960 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "196/196 [==============================] - 57s 292ms/step - loss: 0.6566 - acc: 0.6087 - val_loss: 0.6602 - val_acc: 0.6176 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "196/196 [==============================] - 65s 330ms/step - loss: 0.6473 - acc: 0.6226 - val_loss: 0.6470 - val_acc: 0.6233 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "196/196 [==============================] - 60s 306ms/step - loss: 0.6398 - acc: 0.6311 - val_loss: 0.6503 - val_acc: 0.6294 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "196/196 [==============================] - 61s 313ms/step - loss: 0.6344 - acc: 0.6390 - val_loss: 0.6350 - val_acc: 0.6437 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "196/196 [==============================] - 62s 314ms/step - loss: 0.6281 - acc: 0.6459 - val_loss: 0.6567 - val_acc: 0.6296 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "196/196 [==============================] - 61s 313ms/step - loss: 0.6246 - acc: 0.6509 - val_loss: 0.6239 - val_acc: 0.6535 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "196/196 [==============================] - 62s 314ms/step - loss: 0.6194 - acc: 0.6560 - val_loss: 0.6239 - val_acc: 0.6515 - lr: 0.0010\n",
      "Epoch 9/15\n",
      " 26/196 [==>...........................] - ETA: 47s - loss: 0.6183 - acc: 0.6689"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "\n",
    "# Load the IMDB dataset and split it into training and test sets\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Tokenize the text and convert it to sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "x_train_str = [str(text) for text in x_train]\n",
    "tokenizer.fit_on_texts(x_train_str)\n",
    "x_train = tokenizer.texts_to_sequences(x_train_str)\n",
    "x_test_str = [str(text) for text in x_test]\n",
    "x_test = tokenizer.texts_to_sequences(x_test_str)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "maxlen = 100\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "w2v_model = Word2Vec.load('w2v_model.bin')\n",
    "\n",
    "# Create embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv.key_to_index:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Define early stopping and learning rate reduction callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=15,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8dda72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67d568fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 15s 19ms/step\n",
      "Saved results to CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Get predictions on the test set\n",
    "# Get predicted probabilities on the test set\n",
    "y_pred_prob = model.predict(x_test)\n",
    "\n",
    "# Convert probabilities to classes\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Convert the integer labels to sentiment strings\n",
    "sentiments = ['negative', 'positive']\n",
    "y_test_str = np.array([sentiments[label] for label in y_test])\n",
    "y_pred_str = np.array([sentiments[label] for label in y_pred])\n",
    "\n",
    "# Store the results in a CSV file\n",
    "results = pd.DataFrame({'Review': x_test_str, 'Actual Sentiment': y_test_str, 'Predicted Sentiment': y_pred_str})\n",
    "results.to_csv('imdb_sentiments.csv', index=False)\n",
    "\n",
    "print('Saved results to CSV file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b1ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
